{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate BioRxiv Document Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to generate document embeddings for every article in bioRxiv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T17:40:12.275854Z",
     "start_time": "2020-03-23T17:40:10.243840Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import lxml.etree as ET\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm_notebook\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T17:40:12.400970Z",
     "start_time": "2020-03-23T17:40:12.277170Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_type</th>\n",
       "      <th>heading</th>\n",
       "      <th>category</th>\n",
       "      <th>document</th>\n",
       "      <th>doi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>regular article</td>\n",
       "      <td>new results</td>\n",
       "      <td>genetics</td>\n",
       "      <td>440735_v1.xml</td>\n",
       "      <td>10.1101/440735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>regular article</td>\n",
       "      <td>new results</td>\n",
       "      <td>systems biology</td>\n",
       "      <td>775270_v1.xml</td>\n",
       "      <td>10.1101/775270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>regular article</td>\n",
       "      <td>new results</td>\n",
       "      <td>genetics</td>\n",
       "      <td>242404_v1.xml</td>\n",
       "      <td>10.1101/242404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>regular article</td>\n",
       "      <td>new results</td>\n",
       "      <td>neuroscience</td>\n",
       "      <td>872994_v1.xml</td>\n",
       "      <td>10.1101/2019.12.11.872994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>regular article</td>\n",
       "      <td>new results</td>\n",
       "      <td>developmental biology</td>\n",
       "      <td>080853_v2.xml</td>\n",
       "      <td>10.1101/080853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author_type      heading               category       document  \\\n",
       "0  regular article  new results               genetics  440735_v1.xml   \n",
       "1  regular article  new results        systems biology  775270_v1.xml   \n",
       "2  regular article  new results               genetics  242404_v1.xml   \n",
       "3  regular article  new results           neuroscience  872994_v1.xml   \n",
       "4  regular article  new results  developmental biology  080853_v2.xml   \n",
       "\n",
       "                         doi  \n",
       "0             10.1101/440735  \n",
       "1             10.1101/775270  \n",
       "2             10.1101/242404  \n",
       "3  10.1101/2019.12.11.872994  \n",
       "4             10.1101/080853  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "journal_map_df = pd.read_csv(\"../exploratory_data_analysis/output/biorxiv_article_metadata.tsv\", sep=\"\\t\")\n",
    "journal_map_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Documents to File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section dumps all of biorxiv text into a single document in order to train the word2vec model. This is for ease of training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T14:53:28.225139Z",
     "start_time": "2020-03-20T14:53:26.376Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = ET.XMLParser(encoding='UTF-8', recover=True)\n",
    "\n",
    "# Only use the most current version of the documents\n",
    "latest_journal_version = (\n",
    "    journal_map_df.groupby(\"doi\")\n",
    "    .agg({\"document\":\"first\", \"doi\":\"last\"})\n",
    ")\n",
    "\n",
    "with open(\"output/word2vec_input/biorxiv_text.txt\", \"w\") as f:\n",
    "    for idx, article in tqdm_notebook(latest_journal_version.iterrows()):\n",
    "        tree = (\n",
    "            ET.parse(\n",
    "                open(f\"../biorxiv_articles/{os.path.basename(article['document'])}\", \"rb\"),\n",
    "                parser = parser\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Grab the abstract text\n",
    "        abstract_text = root.xpath(\"//abstract/*//text()\")\n",
    "        abstract_text = list(map(lambda x: remove_stopwords(x), abstract_text))\n",
    "        f.write(\"\".join(abstract_text))\n",
    "        \n",
    "        # Grab the body text\n",
    "        article_text = root.xpath(\"//body/sec/*//text()\")\n",
    "        article_text = list(map(lambda x: remove_stopwords(x), article_text))\n",
    "        f.write(\"\".join(article_text))\n",
    "        \n",
    "        #Sign of a new article\n",
    "        f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section trains the word2vec model (continuous bag of words [CBOW]). Since the number of dimensions can vary I decided to train multiple models: 150, 250, 300. Each model is saved into is own respective directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T14:53:28.225871Z",
     "start_time": "2020-03-20T14:53:26.406Z"
    }
   },
   "outputs": [],
   "source": [
    "class DocIterator: \n",
    "    def __init__(self, filepath): \n",
    "        self.filepath = filepath \n",
    "\n",
    "    def __iter__(self): \n",
    "        for line in open(self.filepath, \"r\"): \n",
    "            yield line.split() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T14:53:28.226529Z",
     "start_time": "2020-03-20T14:53:26.409Z"
    }
   },
   "outputs": [],
   "source": [
    "word_embedding_sizes = [150, 250, 300]\n",
    "for size in word_embedding_sizes:\n",
    "    print(size)\n",
    "    \n",
    "    #Create save path\n",
    "    word_path = Path(f\"output/word2vec_models/{size}\")\n",
    "    word_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Run Word2Vec\n",
    "    words = Word2Vec(DocIterator(\"output/word2vec_input/biorxiv_text.txt\"), size=size, iter=20, seed=100)\n",
    "    \n",
    "    #Save the model for future use\n",
    "    words.save(f\"{str(word_path.resolve())}/biorxiv_{size}.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Document Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the word2vec models, the next step is to generate a document embeddings. For this experiment each document embedding is generated via an average of all word vectors contained in the document. There are better approaches towards doing this, but this can serve as a simple baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T14:53:28.227364Z",
     "start_time": "2020-03-20T14:53:26.436Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_doc_vectors(model, document_df, skip_methods=True):\n",
    "    document_vec_map = {}\n",
    "    parser = ET.XMLParser(encoding='UTF-8', recover=True)\n",
    "\n",
    "    for idx, article in tqdm_notebook(document_df.iterrows()):\n",
    "        tree = (\n",
    "                ET.parse(\n",
    "                    open(f\"../biorxiv_articles/{article['document']}\", \"rb\"),\n",
    "                    parser = parser\n",
    "                )\n",
    "            )\n",
    "        root = tree.getroot()\n",
    "            \n",
    "        word_vectors = []\n",
    "        abstract_text = root.xpath(\"//abstract/*//text()\")\n",
    "            \n",
    "        word_vectors += [\n",
    "            list(\n",
    "                map(\n",
    "                    lambda x: model.wv[x], \n",
    "                    filter(\n",
    "                        lambda x: x in model.wv, \n",
    "                        text.split(\" \")\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            for text in abstract_text\n",
    "        ]\n",
    "            \n",
    "        abstract_vectors = (\n",
    "            list(\n",
    "                itertools.chain.from_iterable(\n",
    "                    filter(lambda x: len(x) > 0, word_vectors)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "            \n",
    "        article_text = root.xpath(\"//body/sec/*//text()\")\n",
    "            \n",
    "        word_vectors += [\n",
    "            list(\n",
    "                map(\n",
    "                    lambda x: model.wv[x], \n",
    "                    filter(\n",
    "                        lambda x: x in model.wv, \n",
    "                        text.split(\" \")\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            for text in article_text\n",
    "        ]\n",
    "            \n",
    "        article_vectors = (\n",
    "            list(\n",
    "                itertools.chain.from_iterable(\n",
    "                    filter(lambda x: len(x) > 0, word_vectors)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "            \n",
    "        total_vectors = abstract_vectors + article_vectors \n",
    "\n",
    "        # skips weird documents that don't contain text\n",
    "        if len(total_vectors) > 0:\n",
    "            document_vec_map[article['document']] = pd.np.stack(total_vectors).mean(axis=0)\n",
    "\n",
    "    return document_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T14:53:28.228114Z",
     "start_time": "2020-03-20T14:53:26.438Z"
    }
   },
   "outputs": [],
   "source": [
    "for word_model_path in Path().rglob(\"output/word2vec_models/*/*.model\"):\n",
    "    model_dim = word_model_path.parents[0].stem\n",
    "    \n",
    "        \n",
    "    word_model = Word2Vec.load(str(word_model_path.resolve()))\n",
    "    biorxiv_vec_map = generate_doc_vectors(word_model, journal_map_df, skip_methods=False)\n",
    "\n",
    "    biorxiv_vec_df = pd.DataFrame([\n",
    "        [document] + biorxiv_vec_map[document].tolist()\n",
    "        for document in biorxiv_vec_map\n",
    "        ], \n",
    "        columns=[\"document\"] + list(map(lambda x: f\"feat_{x}\", range(int(model_dim))))\n",
    "    )\n",
    "    \n",
    "    biorxiv_vec_df.to_csv(\n",
    "        f\"output/word2vec_output/biorxiv_all_articles_{model_dim}.tsv.xz\", \n",
    "        sep=\"\\t\", index=False,\n",
    "        compression=\"xz\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP the Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating document embeddings, the next step is to visualize all the documents. In order to visualize the embeddings a low dimensional representation is needed. UMAP is an algorithm that can generate this representation, while grouping similar embeddings together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T14:53:28.228763Z",
     "start_time": "2020-03-20T14:53:26.467Z"
    }
   },
   "outputs": [],
   "source": [
    "random_state = 100\n",
    "n_neighbors = journal_map_df.category.unique().shape[0]\n",
    "n_components = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T14:53:28.229518Z",
     "start_time": "2020-03-20T14:53:26.469Z"
    }
   },
   "outputs": [],
   "source": [
    "for biorxiv_doc_vectors in Path().rglob(\"output/word2vec_output/biorxiv_all_articles*.tsv.xz\"):\n",
    "    model_dim = int(re.search(r\"(\\d+)\", biorxiv_doc_vectors.stem).group(1))\n",
    "    biorxiv_articles_df = pd.read_csv(str(biorxiv_doc_vectors.resolve()), sep=\"\\t\")\n",
    "    \n",
    "    reducer = umap.UMAP(\n",
    "        n_components=n_components, \n",
    "        n_neighbors=n_neighbors, \n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    embedding = reducer.fit_transform(\n",
    "        biorxiv_articles_df[[f\"feat_{idx}\" for idx in range(model_dim)]].values\n",
    "    )\n",
    "    \n",
    "    umapped_df = (\n",
    "        pd.DataFrame(embedding, columns=[\"umap1\", \"umap2\"])\n",
    "        .assign(document=biorxiv_articles_df.document.values.tolist())\n",
    "        .merge(journal_map_df[[\"category\", \"document\", \"doi\"]], on=\"document\")\n",
    "    )\n",
    "    \n",
    "    umapped_df.to_csv(\n",
    "        f\"output/embedding_output/umap/biorxiv_umap_{model_dim}.tsv\", \n",
    "        sep=\"\\t\", index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE the Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating document embeddings, the next step is to visualize all the documents. In order to visualize the embeddings a low dimensional representation is needed. TSNE is an another algorithm (besides UMAP) that can generate this representation, while grouping similar embeddings together.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T14:53:28.230265Z",
     "start_time": "2020-03-20T14:53:26.495Z"
    }
   },
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "random_state = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T14:53:28.230933Z",
     "start_time": "2020-03-20T14:53:26.498Z"
    }
   },
   "outputs": [],
   "source": [
    "for biorxiv_doc_vectors in Path().rglob(\"output/word2vec_output/biorxiv_all_articles*.tsv.xz\"):\n",
    "    model_dim = int(re.search(r\"(\\d+)\", biorxiv_doc_vectors.stem).group(1))\n",
    "    biorxiv_articles_df = pd.read_csv(str(biorxiv_doc_vectors.resolve()), sep=\"\\t\")\n",
    "    \n",
    "    reducer = TSNE(n_components=n_components, random_state=random_state)\n",
    "    \n",
    "    embedding = reducer.fit_transform(\n",
    "        biorxiv_articles_df[[f\"feat_{idx}\" for idx in range(model_dim)]].values\n",
    "    )\n",
    "    \n",
    "    tsne_df = (\n",
    "        pd.DataFrame(embedding, columns=[\"tsne1\", \"tsne2\"])\n",
    "        .assign(document=biorxiv_articles_df.document.values.tolist())\n",
    "        .merge(journal_map_df[[\"category\", \"document\", \"doi\"]], on=\"document\")\n",
    "    )\n",
    "\n",
    "    tsne_df.to_csv(\n",
    "        f\"output/embedding_output/tsne/biorxiv_tsne_{model_dim}.tsv\", \n",
    "        sep=\"\\t\", index=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T17:40:16.384612Z",
     "start_time": "2020-03-23T17:40:16.382533Z"
    }
   },
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "random_state = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T17:41:04.679449Z",
     "start_time": "2020-03-23T17:40:17.869906Z"
    }
   },
   "outputs": [],
   "source": [
    "for biorxiv_doc_vectors in Path().rglob(\"output/word2vec_output/biorxiv_all_articles*.tsv.xz\"):\n",
    "    model_dim = int(re.search(r\"(\\d+)\", biorxiv_doc_vectors.stem).group(1))\n",
    "    biorxiv_articles_df = pd.read_csv(str(biorxiv_doc_vectors.resolve()), sep=\"\\t\")\n",
    "    \n",
    "    reducer = PCA(\n",
    "        n_components = n_components,\n",
    "        random_state = random_state\n",
    "    )\n",
    "    \n",
    "    embedding = reducer.fit_transform(\n",
    "        biorxiv_articles_df[[f\"feat_{idx}\" for idx in range(model_dim)]].values\n",
    "    )\n",
    "    \n",
    "    pca_df = (\n",
    "        pd.DataFrame(embedding, columns=[\"pca1\", \"pca2\"])\n",
    "        .assign(document=biorxiv_articles_df.document.values.tolist())\n",
    "        .merge(journal_map_df[[\"category\", \"document\", \"doi\"]], on=\"document\")\n",
    "    )\n",
    "    \n",
    "    pca_df.to_csv(\n",
    "        f\"output/embedding_output/pca/biorxiv_pca_{model_dim}.tsv\",\n",
    "        sep=\"\\t\", index=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:annorxiver]",
   "language": "python",
   "name": "conda-env-annorxiver-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
